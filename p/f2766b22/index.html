<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>【LLM】综述 | Rean's Blog</title><noscript>开启JavaScript才能访问本站哦~</noscript><link rel="icon" href="/img/favicon.png"><!-- index.css--><link rel="stylesheet" href="/css/index.css?v=1.8.10"><!-- inject head--><link rel="stylesheet" href="https://cdn2.codesign.qq.com/icons/7pOrz0WXB5ZWJPX/latest/iconfont.css"><!-- aplayer--><link rel="stylesheet" href="https://cdn.staticfile.net/aplayer/1.10.1/APlayer.min.css"><!-- swiper--><link rel="stylesheet" href="https://cdn.staticfile.net/Swiper/11.0.5/swiper-bundle.min.css"><!-- fancybox ui--><link rel="stylesheet" href="https://cdn.staticfile.net/fancyapps-ui/5.0.36/fancybox/fancybox.min.css"><!-- katex--><!-- Open Graph--><meta name="description" content="大模型进化史 Transformer革命（2017年） Transformer（2017）起源与发展Transformer 源自于自然语言处理任务NLP；在计算机视觉领域CV，近年来Transformer逐渐替代CNN成为一个热门的研究方向。此外，Transformer在文本、语音、视频等多模态"><!-- pwa--><meta name="apple-mobile-web-app-capable" content="Rean's Blog"><meta name="theme-color" content="var(--efu-main)"><meta name="apple-mobile-web-app-status-bar-style" content="var(--efu-main)"><link rel="bookmark" href="/img/favicon.png"><link rel="apple-touch-icon" href="/img/favicon.png" sizes="180x180"><script>console.log(
    "%c Program: Hexo %c Theme: Solitude %c Version: v1.8.10",
    "border-radius:5px 0 0 5px;padding: 5px 10px;color:white;background:#ff3842;",
    "padding: 5px 10px;color:white;background:#3e9f50;",
    "padding: 5px 10px;color:white;background:#0084ff;",
)
</script><script>(()=>{
        const saveToLocal = {
            set: function setWithExpiry(key, value, ttl) {
                if (ttl === 0)
                    return
                const now = new Date()
                const expiryDay = ttl * 86400000
                const item = {
                    value: value,
                    expiry: now.getTime() + expiryDay
                }
                localStorage.setItem(key, JSON.stringify(item))
            },
            get: function getWithExpiry(key) {
                const itemStr = localStorage.getItem(key)
    
                if (!itemStr) {
                    return undefined
                }
                const item = JSON.parse(itemStr)
                const now = new Date()
    
                if (now.getTime() > item.expiry) {
                    localStorage.removeItem(key)
                    return undefined
                }
                return item.value
            }
        };
        window.utils = {
            saveToLocal: saveToLocal,
            getCSS: (url, id = false) => new Promise((resolve, reject) => {
              const link = document.createElement('link')
              link.rel = 'stylesheet'
              link.href = url
              if (id) link.id = id
              link.onerror = reject
              link.onload = link.onreadystatechange = function() {
                const loadState = this.readyState
                if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
                link.onload = link.onreadystatechange = null
                resolve()
              }
              document.head.appendChild(link)
            }),
            getScript: (url, attr = {}) => new Promise((resolve, reject) => {
              const script = document.createElement('script')
              script.src = url
              script.async = true
              script.onerror = reject
              script.onload = script.onreadystatechange = function() {
                const loadState = this.readyState
                if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
                script.onload = script.onreadystatechange = null
                resolve()
              }
    
              Object.keys(attr).forEach(key => {
                script.setAttribute(key, attr[key])
              })
    
              document.head.appendChild(script)
            }),
            addGlobalFn: (key, fn, name = false, parent = window) => {
                const globalFn = parent.globalFn || {}
                const keyObj = globalFn[key] || {}
        
                if (name && keyObj[name]) return
        
                name = name || Object.keys(keyObj).length
                keyObj[name] = fn
                globalFn[key] = keyObj
                parent.globalFn = globalFn
            },
        }
    })()</script><!-- global head--><script>const GLOBAL_CONFIG = {
    root: '/',
    algolia: undefined,
    localsearch: {"preload":false,"path":"/search.xml"},
    runtime: '2024-01-05 00:00:00',
    lazyload: {
        enable: true,
        error: '/img/error_load.png'
    },
    copyright: {"limit":50,"author":"作者: Rean","link":"链接: ","source":"来源: Rean's Blog","info":"著作权归作者所有。 商业转载请联系作者获得授权，非商业转载请注明出处。"},
    highlight: {
        enable: true,
        limit: 200,
        expand: true,
        copy: true,
        syntax: 'highlight.js'
    },
    randomlink: false,
    lang: {"theme":{"dark":"已切换至深色模式","light":"已切换至浅色模式"},"copy":{"success":"复制成功","error":"复制失败"},"backtop":"返回顶部","time":{"day":"天前","hour":"小时前","just":"刚刚","min":"分钟前","month":"个月前"},"f12":"开发者模式已打开，请遵循GPL协议。","totalk":"无需删除空行，直接输入评论即可","search":{"empty":"找不到你查询的内容：${query}","hit":"找到 ${hits} 条结果，用时 ${time} 毫秒","placeholder":"输入关键词快速查找","count":"共 <b>${count}</b> 条结果。"}},
    aside: {
        sayhello: {
            morning: '一日之计在于晨',
            noon: '吃饱了才有力气干活',
            afternoon: '集中精力，攻克难关',
            night: '不要太劳累了，早睡更健康',
            goodnight: '睡个好觉，保证精力充沛',
        },
        sayhello2: ["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🔨 设计开发一条龙","🎮 游戏爱好者"],
    },
    covercolor: {
        enable: true
    },
    comment: false,
    lightbox: 'fancybox',
    post_ai: false,
    right_menu: {"mode":{"dark":"深色模式","light":"浅色模式"},"img_error":"此图片无法复制与下载","music":{"start":"播放音乐","stop":"暂停音乐"},"translate":{"translateDelay":0,"defaultEncoding":2}},
};</script><!-- page-config head--><script id="config-diff">var PAGE_CONFIG = {
    is_post: true,
    is_page: false,
    is_home: false,
    page: '',
    toc: true,
    comment: false,
    ai_text: false,
}</script><meta name="generator" content="Hexo 7.2.0"></head><body id="body"><!-- universe--><canvas id="universe"></canvas><!-- loading--><!-- console--><div id="console"><div class="close-btn" onclick="sco.hideConsole()"><i class="solitude st-close-fill"></i></div><div class="button-group"><div class="console-btn-item"><span class="darkmode_switchbutton" onclick="sco.switchDarkMode()" title="昼夜切换"><i class="solitude st-moon-clear-fill"></i></span></div><div class="console-btn-item" id="consoleHideAside"><span class="asideSwitch" onclick="sco.switchHideAside()" title="边栏显示控制"><i class="solitude st-side-bar-fill"></i></span></div><div class="console-btn-item" id="consoleMusic" onclick="sco.musicToggle()"><span class="music-switch" title="音乐开关"><i class="solitude st-disc-fill"></i></span></div></div><div class="console-mask" onclick="sco.hideConsole()"></div></div><!-- sidebar--><div id="sidebar" style="zoom: 1;"><div id="menu-mask" style="display: none;"></div><div id="sidebar-menus"><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">182</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">160</div></a></div></div></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><span class="darkmode_switchbutton menu-child" onclick="sco.switchDarkMode()"><i class="solitude st-moon-clear-fill"></i><span>显示模式</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>文章</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="solitude  st-folder-fill"></i><span>文章列表</span></a></li><li><a class="site-page child" href="/categories/"><i class="solitude  st-checkbox-multiple-blank-fill"></i><span>分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="solitude  st-price-tag-fill"></i><span>标签</span></a></li><li><a class="site-page child" href="/study/"><i class="solitude  st-gift-fill"></i><span>学习</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>我的</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="solitude  st-disc-fill"></i><span>音乐馆</span></a></li><li><a class="site-page child" href="/essay/"><span>即刻短文</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>关于</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="solitude  st-contacts-fill"></i><span>关于本站</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-widget card-tags card-archives card-webinfo card-allinfo"><div class="card-tag-cloud"><a href="/tags/%E8%85%BE%E8%AE%AF/">腾讯<sup>1</sup></a><a href="/tags/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91/">后台开发<sup>1</sup></a><a href="/tags/Markdown/">Markdown<sup>1</sup></a><a href="/tags/Butterfly/">Butterfly<sup>2</sup></a><a href="/tags/PyCharm/">PyCharm<sup>1</sup></a><a href="/tags/IDEA/">IDEA<sup>1</sup></a><a href="/tags/Git/">Git<sup>2</sup></a><a href="/tags/Halo/">Halo<sup>1</sup></a><a href="/tags/GitHub/">GitHub<sup>1</sup></a><a href="/tags/JavaFx/">JavaFx<sup>2</sup></a><a href="/tags/SpringBoot/">SpringBoot<sup>7</sup></a><a href="/tags/Java/">Java<sup>18</sup></a><a href="/tags/MyBatis/">MyBatis<sup>4</sup></a><a href="/tags/Hexo/">Hexo<sup>3</sup></a><a href="/tags/SyncTV/">SyncTV<sup>1</sup></a><a href="/tags/AList/">AList<sup>1</sup></a><a href="/tags/%E5%AE%9D%E5%A1%94/">宝塔<sup>3</sup></a><a href="/tags/Vue/">Vue<sup>18</sup></a><a href="/tags/Vite/">Vite<sup>6</sup></a><a href="/tags/JavaScript/">JavaScript<sup>4</sup></a><a href="/tags/TypeScript/">TypeScript<sup>3</sup></a><a href="/tags/Electron/">Electron<sup>4</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习<sup>1</sup></a><a href="/tags/scikit-learn/">scikit-learn<sup>1</sup></a><a href="/tags/vocalremover/">vocalremover<sup>1</sup></a><a href="/tags/BPM/">BPM<sup>1</sup></a><a href="/tags/%E9%BB%91%E7%A5%9E%E8%AF%9D%EF%BC%9A%E6%82%9F%E7%A9%BA/">黑神话：悟空<sup>1</sup></a><a href="/tags/HTTP/">HTTP<sup>5</sup></a><a href="/tags/HTTPS/">HTTPS<sup>2</sup></a><a href="/tags/%E7%9F%AD%E9%93%BE/">短链<sup>1</sup></a><a href="/tags/%E8%A7%86%E9%A2%91/">视频<sup>1</sup></a><a href="/tags/HTTP-1-1/">HTTP/1.1<sup>1</sup></a><a href="/tags/LaTex/">LaTex<sup>1</sup></a><a href="/tags/%E5%8A%A0%E5%AF%86/">加密<sup>1</sup></a><a href="/tags/IP/">IP<sup>2</sup></a><a href="/tags/ICMP/">ICMP<sup>1</sup></a><a href="/tags/I-O/">I/O<sup>2</sup></a><a href="/tags/JVM/">JVM<sup>2</sup></a><a href="/tags/Springboot/">Springboot<sup>1</sup></a><a href="/tags/ThreadLocal/">ThreadLocal<sup>1</sup></a><a href="/tags/UUID/">UUID<sup>1</sup></a><a href="/tags/%E5%B9%B6%E5%8F%91/">并发<sup>4</sup></a><a href="/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/">红黑树<sup>2</sup></a><a href="/tags/%E9%93%BE%E8%A1%A8/">链表<sup>1</sup></a><a href="/tags/%E5%8A%9B%E6%89%A3/">力扣<sup>1</sup></a><a href="/tags/LeetCode/">LeetCode<sup>1</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库<sup>8</sup></a><a href="/tags/NLP/">NLP<sup>6</sup></a><a href="/tags/spaCy/">spaCy<sup>4</sup></a><a href="/tags/Python/">Python<sup>1</sup></a><a href="/tags/Redis/">Redis<sup>10</sup></a><a href="/tags/RDB/">RDB<sup>1</sup></a><a href="/tags/AOF/">AOF<sup>1</sup></a><a href="/tags/%E7%BA%BF%E7%A8%8B/">线程<sup>2</sup></a><a href="/tags/I-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/">I/O多路复用<sup>2</sup></a><a href="/tags/Spring/">Spring<sup>5</sup></a><a href="/tags/TCP/">TCP<sup>7</sup></a><a href="/tags/UDP/">UDP<sup>1</sup></a><a href="/tags/Socket/">Socket<sup>1</sup></a><a href="/tags/FFMpeg/">FFMpeg<sup>1</sup></a><a href="/tags/%E6%A0%B7%E5%BC%8F/">样式<sup>1</sup></a><a href="/tags/ECMAScript/">ECMAScript<sup>2</sup></a><a href="/tags/Element/">Element<sup>3</sup></a><a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/">分布式锁<sup>1</sup></a><a href="/tags/%E5%91%A8%E6%B7%B1/">周深<sup>7</sup></a><a href="/tags/%E5%BE%B5%E7%BE%BD%E6%91%A9%E6%9F%AF/">徵羽摩柯<sup>1</sup></a><a href="/tags/%E9%9F%B3%E4%B9%90%E7%BC%98%E8%AE%A1%E5%88%92/">音乐缘计划<sup>4</sup></a><a href="/tags/%E5%A5%A5%E6%96%AF%E5%8D%A1/">奥斯卡<sup>3</sup></a><a href="/tags/%E5%BC%A0%E6%9D%B0/">张杰<sup>2</sup></a><a href="/tags/%E5%A3%B0%E7%94%9F%E4%B8%8D%E6%81%AF%C2%B7%E5%AE%9D%E5%B2%9B%E5%AD%A3/">声生不息·宝岛季<sup>3</sup></a><a href="/tags/%E5%BE%90%E4%BD%B3%E8%8E%B9/">徐佳莹<sup>1</sup></a><a href="/tags/SynthV/">SynthV<sup>22</sup></a><a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统<sup>4</sup></a><a href="/tags/%E6%B0%B8%E5%A4%9CMinus/">永夜Minus<sup>6</sup></a><a href="/tags/%E9%9F%A6%E7%A4%BC%E5%AE%89/">韦礼安<sup>1</sup></a><a href="/tags/%E8%94%A1%E4%BE%9D%E6%9E%97/">蔡依林<sup>1</sup></a><a href="/tags/%E6%97%A0%E5%8F%82/">无参<sup>46</sup></a><a href="/tags/%E5%8F%A4%E9%A3%8E/">古风<sup>12</sup></a><a href="/tags/%E4%B8%89%E6%97%A0Marblue/">三无Marblue<sup>1</sup></a><a href="/tags/%E7%B4%A2%E5%BC%95/">索引<sup>1</sup></a><a href="/tags/B%E6%A0%91/">B树<sup>1</sup></a><a href="/tags/B-%E6%A0%91/">B+树<sup>1</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构<sup>1</sup></a><a href="/tags/%E6%A0%91/">树<sup>1</sup></a><a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/">二叉树<sup>1</sup></a><a href="/tags/%E8%AE%B8%E9%92%A7/">许钧<sup>1</sup></a><a href="/tags/%E6%AD%8C%E6%89%8B2024/">歌手2024<sup>37</sup></a><a href="/tags/Chante-Moore/">Chanté Moore<sup>2</sup></a><a href="/tags/%E9%BB%84%E5%AE%A3/">黄宣<sup>1</sup></a><a href="/tags/Faouzia/">Faouzia<sup>4</sup></a><a href="/tags/Lenka/">Lenka<sup>1</sup></a><a href="/tags/%E5%BC%A0%E9%92%B0%E7%90%AA/">张钰琪<sup>1</sup></a><a href="/tags/%E9%82%A3%E8%8B%B1/">那英<sup>9</sup></a><a href="/tags/%E5%B0%A4%E9%95%BF%E9%9D%96/">尤长靖<sup>2</sup></a><a href="/tags/%E5%AD%99%E6%A5%A0/">孙楠<sup>5</sup></a><a href="/tags/%E8%B0%AD%E7%BB%B4%E7%BB%B4/">谭维维<sup>3</sup></a><a href="/tags/%E5%B4%A9%E5%9D%8F%EF%BC%9A%E6%98%9F%E7%A9%B9%E9%93%81%E9%81%93/">崩坏：星穹铁道<sup>4</sup></a><a href="/tags/%E8%B5%A4%E7%BE%BD/">赤羽<sup>1</sup></a><a href="/tags/%E6%B1%AA%E8%8B%8F%E6%B3%B7/">汪苏泷<sup>4</sup></a><a href="/tags/%E5%A3%B0%E7%94%9F%E4%B8%8D%E6%81%AF%C2%B7%E5%AE%B6%E5%B9%B4%E5%8D%8E/">声生不息·家年华<sup>3</sup></a><a href="/tags/%E9%99%88%E6%A5%9A%E7%94%9F/">陈楚生<sup>3</sup></a><a href="/tags/%E9%99%88%E4%BA%A6%E6%B4%BA/">陈亦洺<sup>1</sup></a><a href="/tags/%E5%B0%9A%E8%BE%B0/">尚辰<sup>1</sup></a><a href="/tags/%E5%BF%98%E5%B7%9D%E9%A3%8E%E5%8D%8E%E5%BD%95/">忘川风华录<sup>2</sup></a><a href="/tags/%E4%B9%90%E6%AD%A3%E9%BE%99%E7%89%99/">乐正龙牙<sup>1</sup></a><a href="/tags/%E6%98%9F%E5%B0%98Infinity/">星尘Infinity<sup>4</sup></a><a href="/tags/%E6%98%8E%E6%97%A5%E6%96%B9%E8%88%9F/">明日方舟<sup>3</sup></a><a href="/tags/%E7%A5%96%E5%A8%85%E7%BA%B3%E6%83%9C/">祖娅纳惜<sup>1</sup></a><a href="/tags/%E7%94%B5%E5%AD%90/">电子<sup>1</sup></a><a href="/tags/%E6%98%9F%E5%B0%98/">星尘<sup>6</sup></a><a href="/tags/%E5%BC%A0%E4%BF%A1%E5%93%B2/">张信哲<sup>2</sup></a><a href="/tags/%E9%93%B6%E4%B8%B4/">银临<sup>1</sup></a><a href="/tags/%E6%9D%8E%E5%B8%B8%E8%B6%85/">李常超<sup>1</sup></a><a href="/tags/%E5%B1%B1%E8%89%B2%E6%9C%89%E6%97%A0%E4%B8%AD/">山色有无中<sup>1</sup></a><a href="/tags/%E5%91%A8%E6%9D%B0%E4%BC%A6/">周杰伦<sup>1</sup></a><a href="/tags/%E9%BB%84%E9%9C%84%E9%9B%B2/">黄霄雲<sup>1</sup></a><a href="/tags/%E5%AE%B9%E7%A5%96%E5%84%BF/">容祖儿<sup>1</sup></a><a href="/tags/%E6%88%91%E6%98%AF%E6%AD%8C%E6%89%8B%C2%B7%E7%AC%AC%E5%9B%9B%E5%AD%A3/">我是歌手·第四季<sup>1</sup></a><a href="/tags/%E6%9D%8E%E5%85%8B%E5%8B%A4/">李克勤<sup>1</sup></a><a href="/tags/%E7%B2%A4%E8%AF%AD/">粤语<sup>2</sup></a><a href="/tags/%E9%BB%84%E4%B8%BD%E7%8E%B2/">黄丽玲<sup>1</sup></a><a href="/tags/G-E-M-%E9%82%93%E7%B4%AB%E6%A3%8B/">G.E.M.邓紫棋<sup>7</sup></a><a href="/tags/T-I-M-E/">T.I.M.E.<sup>4</sup></a><a href="/tags/%E6%97%B6%E5%85%89%E9%9F%B3%E4%B9%90%E4%BC%9A%C2%B7%E8%80%81%E5%8F%8B%E8%AE%B0/">时光音乐会·老友记<sup>5</sup></a><a href="/tags/%E5%BC%A0%E5%AD%A6%E5%8F%8B/">张学友<sup>1</sup></a><a href="/tags/%E6%9D%A8%E4%B8%9E%E7%90%B3/">杨丞琳<sup>1</sup></a><a href="/tags/%E6%B1%AA%E5%B3%B0/">汪峰<sup>1</sup></a><a href="/tags/%E6%9C%AA%E5%AE%9A%E4%BA%8B%E4%BB%B6%E7%B0%BF/">未定事件簿<sup>1</sup></a><a href="/tags/%E5%BC%A0%E8%BF%9C/">张远<sup>1</sup></a><a href="/tags/%E5%AD%99%E7%87%95%E5%A7%BF/">孙燕姿<sup>2</sup></a><a href="/tags/%E6%B5%B7%E6%9D%A5%E9%98%BF%E6%9C%A8/">海来阿木<sup>1</sup></a><a href="/tags/%E6%9B%B9%E6%9D%A8/">曹杨<sup>3</sup></a><a href="/tags/%E4%B8%8B%E4%B8%80%E6%88%98%E6%AD%8C%E6%89%8B/">下一战歌手<sup>3</sup></a><a href="/tags/Fine%E4%B9%90%E5%9B%A2/">Fine乐团<sup>1</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">计算机网络<sup>4</sup></a><a href="/tags/DNS/">DNS<sup>1</sup></a><a href="/tags/MAC/">MAC<sup>1</sup></a><a href="/tags/Adam-Lambert/">Adam Lambert<sup>1</sup></a><a href="/tags/%E4%BA%9A%E5%BD%93%C2%B7%E5%85%B0%E4%BC%AF%E7%89%B9/">亚当·兰伯特<sup>1</sup></a><a href="/tags/Hayden/">Hayden<sup>1</sup></a><a href="/tags/%E6%9A%AE%E5%A4%9C/">暮夜<sup>1</sup></a><a href="/tags/ACE-Studio/">ACE Studio<sup>3</sup></a><a href="/tags/%E5%B0%9A%E9%9B%AF%E5%A9%95/">尚雯婕<sup>1</sup></a><a href="/tags/%E4%B8%9C%E6%96%B9%E6%A0%80%E5%AD%90/">东方栀子<sup>2</sup></a><a href="/tags/%E9%BB%84%E7%BB%AE%E7%8F%8A/">黄绮珊<sup>1</sup></a><a href="/tags/%E5%8D%8E%E6%99%A8%E5%AE%87/">华晨宇<sup>1</sup></a><a href="/tags/JUN/">JUN<sup>1</sup></a><a href="/tags/%E6%9E%97%E5%BF%86%E8%8E%B2/">林忆莲<sup>1</sup></a><a href="/tags/UTAU/">UTAU<sup>1</sup></a><a href="/tags/Sheena/">Sheena<sup>1</sup></a><a href="/tags/%E5%89%A7%E5%A5%BD%E5%90%AC%E7%9A%84%E6%AD%8C/">剧好听的歌<sup>1</sup></a><a href="/tags/Syncthing/">Syncthing<sup>1</sup></a><a href="/tags/%E5%90%8C%E6%AD%A5/">同步<sup>1</sup></a><a href="/tags/%E6%94%AF%E4%BB%98%E5%AE%9D/">支付宝<sup>1</sup></a><a href="/tags/Apifox/">Apifox<sup>2</sup></a><a href="/tags/%E5%B2%B8%E6%99%93/">岸晓<sup>1</sup></a><a href="/tags/%E5%8D%95%E4%BE%9D%E7%BA%AF/">单依纯<sup>2</sup></a><a href="/tags/%E9%BB%84%E5%AD%90%E5%BC%98%E5%87%A1/">黄子弘凡<sup>1</sup></a><a href="/tags/%E5%A2%A8%E6%B8%85%E5%BC%A6/">墨清弦<sup>1</sup></a><a href="/tags/%E6%AD%8C%E6%89%8B2025/">歌手2025<sup>1</sup></a></div></div></div></div><!-- keyboard--><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav class="show" id="nav"><div id="nav-group"><div id="blog_name"><a id="site-name" href="/" title="返回博客主页"><span class="title">Rean's Blog</span></a></div><div id="page-name-mask"><div id="page-name"><a id="page-name-text" onclick="sco.toTop()">【LLM】综述</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>文章</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="solitude  st-folder-fill"></i><span>文章列表</span></a></li><li><a class="site-page child" href="/categories/"><i class="solitude  st-checkbox-multiple-blank-fill"></i><span>分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="solitude  st-price-tag-fill"></i><span>标签</span></a></li><li><a class="site-page child" href="/study/"><i class="solitude  st-gift-fill"></i><span>学习</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>我的</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="solitude  st-disc-fill"></i><span>音乐馆</span></a></li><li><a class="site-page child" href="/essay/"><span>即刻短文</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>关于</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="solitude  st-contacts-fill"></i><span>关于本站</span></a></li></ul></div></div></div><div id="nav-left"></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机一篇文章" href="javascript:void(0);"><i class="solitude st-signal-tower-fill"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索"><i class="solitude st-search-line"></i></a></div><div class="nav-button" id="nav-console"><a class="console_switchbutton" onclick="sco.showConsole()" title="中控台" href="javascript:void(0);"><i class="solitude st-dashboard-fill"></i></a></div><div class="nav-button" id="nav-totop" onclick="sco.toTop()"><a class="totopbtn"><i class="solitude st-arrow-up-line"></i><span id="percent">0</span></a></div><div id="toggle-menu"><a class="site-page"><i class="solitude st-menu-line"></i></a></div></div></div></nav><div class="coverdiv" id="coverdiv"><img class="nolazyload" id="post-cover" src="https://npm.elemecdn.com/justlovesmile-photo/cover5.JPG" alt="【LLM】综述"></div><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original" title="该文章为原创文章，注意版权协议">原创</a><span class="post-meta-categories"><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span><div class="tag_share"><div class="post-meta__tag-list"></div></div></div></div><h1 class="post-title">【LLM】综述</h1><div id="post-meta"><div class="meta-secondline"><span class="post-meta-date" title="发布于 2025-07-24 11:16:47"><i class="post-meta-icon solitude st-calendar-todo-fill"></i><time datetime="2025-07-24T03:16:47.000Z">2025-07-24T03:16:47.000Z</time></span><span class="post-meta-date" title="最后更新于 2025-07-24 18:01:23"><i class="post-meta-icon solitude st-refresh-line"></i><time datetime="2025-07-24T10:01:23.116Z">2025-07-24T10:01:23.116Z</time></span><span class="post-meta-wordcount"><i class="post-meta-icon solitude st-word-fill" title="文章字数"></i><span class="word-count">4.7k</span><span class="post-meta-separator"></span><i class="post-meta-icon solitude st-clock-fill" title="阅读耗时"></i><span>17 min</span></span><a class="post-meta-pv" href="/p/f2766b22/" title="文章热度"><i class="post-meta-icon solitude st-fire-fill"></i><span id="busuanzi_page_pv"><i class="solitude st-loading-line"></i></span></a></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="大模型进化史"><a href="#大模型进化史" class="headerlink" title="大模型进化史"></a>大模型进化史</h1><p><img src= "/img/spin.svg" data-lazy-src="https://rean-blog-bucket.oss-cn-guangzhou.aliyuncs.com/assets/essay/v2-ac0880235f3346cf0bd0a94c84cda004_1440w.png"></p>
<div class="note  success flat"><p>Transformer革命（2017年）</p>
</div>

<h2 id="Transformer（2017）"><a href="#Transformer（2017）" class="headerlink" title="Transformer（2017）"></a>Transformer（2017）</h2><h3 id="起源与发展"><a href="#起源与发展" class="headerlink" title="起源与发展"></a>起源与发展</h3><p>Transformer 源自于自然语言处理任务NLP；在计算机视觉领域CV，近年来Transformer逐渐替代CNN成为一个热门的研究方向。此外，Transformer在文本、语音、视频等多模态领域也在崭露头角。</p>
<p>2017 年 Google 在<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">《Attention Is All You Need》</a>中提出了 Transformer 结构用于序列标注，在翻译任务上超过了之前最优秀的循环神经网络模型；与此同时，Fast AI 在<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.06146">《Universal Language Model Fine-tuning for Text Classification》</a>中提出了一种名为 ULMFiT 的迁移学习方法，将在大规模数据上预训练好的 LSTM 模型迁移用于文本分类，只用很少的标注数据就达到了最佳性能。</p>
<p>这些具有开创性的工作促成了两个著名 Transformer 模型的出现：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://openai.com/blog/language-unsupervised/"><strong>GPT</strong></a> (the Generative Pretrained Transformer)；</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805"><strong>BERT</strong></a> (Bidirectional Encoder Representations from Transformers)。</li>
</ul>
<p>Transformer 和 LSTM（Long Short-term Memory Networks，长短时记忆神经网络）的最大区别，就是 LSTM 的训练是迭代的、<strong>串行</strong>的，必须要等当前字处理完，才可以处理下一个字；而 Transformer 的训练是<strong>并行</strong>的，即所有字是同时训练的，这样就大大增加了计算效率。Transformer 使用了位置嵌入（Positional Encoding）来理解语言的顺序，使用<strong>自注意力机制</strong>（Self Attention Mechanism）和<strong>前馈全连接层</strong>进行计算，这是 Transformer 的创新点。</p>
<h3 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h3><p><img src= "/img/spin.svg" data-lazy-src="https://rean-blog-bucket.oss-cn-guangzhou.aliyuncs.com/assets/essay/d3e2a9ca5dbb5c621067b4cc65e0df30.png"></p>
<p>在 Transformer 模型中，<strong>编码器负责理解和提取输入文本中的相关信息</strong>，通常涉及到处理文本的序列化形式，例如单词或字符，并且用<strong>自注意力机制</strong>（Self-Attention）来理解文本中的上下文关系。</p>
<p><strong>编码器的输出是输入文本的连续表示</strong>，通常称为<strong>嵌入</strong>（Embedding）。这种嵌入包含了编码器从文本中提取的所有有用信息，并以一种可以被模型处理的格式（通常是高维向量）表示。</p>
<p><strong>解码器的任务是根据从编码器接收到的嵌入来生成翻译后的文本（目标语言）</strong>，其中<strong>编码器是双向</strong>的，<strong>解码器是单向</strong>的，需要循环迭代输出。</p>
<p>虽然新的 Transformer 模型层出不穷，它们采用不同的预训练目标在不同的数据集上进行训练，但是依然可以按模型结构将它们大致分为三类：</p>
<ul>
<li><strong>纯 Encoder (Encoder-Only) 模型</strong>（例如 BERT），又称自编码 (auto-encoding) Transformer 模型；</li>
<li><strong>纯 Decoder (Decoder-Only) 模型</strong>（例如 GPT），又称自回归 (auto-regressive) Transformer 模型；</li>
<li><strong>Encoder-Decoder 模型</strong>（例如 BART、T5），又称 Seq2Seq (sequence-to-sequence) Transformer 模型。</li>
</ul>
<p><img src= "/img/spin.svg" data-lazy-src="https://rean-blog-bucket.oss-cn-guangzhou.aliyuncs.com/assets/essay/20250724131959.png"></p>
<h3 id="Encoder-Only"><a href="#Encoder-Only" class="headerlink" title="Encoder-Only"></a>Encoder-Only</h3><p><strong>Encoder-only <strong>架构的 LLMs 仅包含编码器部分。它主要适用于不需要生成序列的任务，只需要对输入进行编码和处理的单向任务场景，更擅长</strong>对文本内容进行分析、分类</strong>，包括情感分析，命名实体识别。</p>
<p>Encoder-Only 架构的核心思想是利用神经网络<strong>对输入文本进行编码，提取其特征和语义信息</strong>，并将编码结果传递给后续的处理模块。这种架构的优点是能够更好地理解输入文本的语义和上下文信息，从而提高文本分类和情感分析等任务的准确性。缺点是它<strong>无法直接生成文本输出</strong>，因此在需要生成文本的任务中不太适用。</p>
<p>Encoder-Only 架构的大模型有谷歌的 BERT、智谱AI发布的第四代基座大语言模型 GLM4（2024年）等。其中，BERT是基于Encoder-Only架构的预训练语言模型。GLM4是智谱AI发布的第四代基座大语言模型，该模型在IFEval评测集上，在Prompt提示词跟随（中文）方面，GLM-4达到了GPT-4 88%的水平。</p>
<h3 id="Decoder-Only"><a href="#Decoder-Only" class="headerlink" title="Decoder-Only"></a>Decoder-Only</h3><p>Decoder-Only 架构，也被称为生成式架构，仅包含解码器部分，主要是为了预测下一个输出的内容 &#x2F; token 是什么，并把之前输出的内容 &#x2F; token 作为上下文学习。它通常用于序列生成任务，如文本生成、机器翻译等。这种架构的模型适用于需要生成序列的任务，可以从输入的编码中生成相应的序列。同时，Decoder-Only 架构还有一个重要特点是可以进行无监督预训练。在预训练阶段，模型通过大量的无标注数据学习语言的统计模式和语义信息。</p>
<p>Decoder-Only 架构的优点是擅长创造性的写作，比如写小说或自动生成文章。它更多关注于从已有的信息（开头）扩展出新的内容。其缺点是<strong>需要大量的训练数据</strong>来提高生成文本的质量和多样性。</p>
<p>Decoder-Only 架构的大模型的代表有 GPT 系列、LLaMA、OPT、BLOOM 等。这类模型采用预测下一个词进行训练，常见下游任务有文本生成、问答等，因此被称为 ALM（Autoregressive Language Model）。</p>
<h3 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h3><p>Encoder-Decoder 架构，也被称为序列到序列架构，同时包含编码器和解码器部分。它通常用于序列到序列（Seq2Seq）任务，如机器翻译、对话生成等。这种架构的代表是以 Google 训练出来的 T5 为代表的相关大模型。</p>
<p>Encoder-Decoder 架构的核心思想是利用编码器对输入序列进行编码，提取其特征和语义信息，并将编码结果传递给解码器。然后，解码器根据编码结果生成相应的输出序列。这种架构的优点是能够更好地处理输入序列和输出序列之间的关系，从而提高机器翻译和对话生成等任务的准确性。缺点是<strong>模型复杂度较高，训练时间和计算资源消耗较大</strong>。</p>
<p>Encoder-Decoder 架构的大模型有很多，例如 Google 的 T5 模型、华为的盘古 NLP 大模型等。</p>
<p>其中，华为的盘古 NLP 大模型首次使用 Encoder-Decoder 架构，兼顾 NLP 大模型的理解能力和生成能力，保证了模型在不同系统中的嵌入灵活性。在下游应用中，仅需少量样本和可学习参数即可完成千亿规模大模型的快速微调和下游适配，这一模型在智能舆论以及智能营销方面都有不错的表现。</p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li>自注意力机制（Self-Attention）：这是Transformer的核心概念之一，它使模型能够同时考虑输入序列中的所有位置，而不是像循环神经网络（RNN）或卷积神经网络（CNN）一样逐步处理。自注意力机制允许模型根据输入序列中的不同部分来赋予不同的注意权重，从而更好地捕捉语义关系。</li>
<li>多头注意力（Multi-Head Attention）：Transformer中的<strong>自注意力机制被扩展为多个注意力头</strong>，每个头可以学习不同的注意权重，以更好地捕捉不同类型的关系。多头注意力允许模型并行处理不同的信息子空间。</li>
<li>堆叠层（Stacked Layers）：Transformer通常由多个相同的编码器和解码器层堆叠而成。这些堆叠的层有助于模型学习复杂的特征表示和语义。</li>
<li>位置编码（Positional Encoding）：由于Transformer没有内置的序列位置信息，它需要额外的位置编码来表达输入序列中单词的位置顺序。</li>
<li>残差连接和层归一化（Residual Connections and Layer Normalization）：这些技术有助于减轻训练过程中的梯度消失和爆炸问题，使模型更容易训练。</li>
<li>编码器和解码器：Transformer通常包括一个编码器用于处理输入序列和一个解码器用于生成输出序列，这使其适用于序列到序列的任务，如机器翻译。</li>
</ul>
<div class="note  success flat"><p>预训练Transformer模型时代（2018 - 2020年）</p>
</div>

<p>2017 年 Transformer 架构的引入为自然语言处理开启了一个新时代，其特点是预训练模型的兴起以及对模型规模前所未有的关注。这一时期出现了两个具有影响力的模型家族：BERT 和 GPT，它们展示了大规模预训练和微调范式的强大力量。</p>
<h2 id="BERT（2018-10）"><a href="#BERT（2018-10）" class="headerlink" title="BERT（2018.10）"></a>BERT（2018.10）</h2><p><img src= "/img/spin.svg" data-lazy-src="https://rean-blog-bucket.oss-cn-guangzhou.aliyuncs.com/assets/essay/20250724161234.png"></p>
<div class="note  info flat"><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
</div>

<p>2018年，谷歌推出了 BERT（Bidirectional Encoder Representations from Transformers，双向上下文理解），它使用 Transformer 的<strong>编码器（Encoder-Only）</strong>，与之前单向处理文本（从左到右或从右到左）的模型不同，BERT 采用了<strong>双向（Bidirectional）</strong>训练方法，使其能够<strong>同时从两个方向捕捉上下文</strong>。</p>
<p>例如：“The bank is situated on the ______ of the river.”</p>
<p>在单向模型中，对空白的理解将<strong>严重依赖于前面的单词</strong>，并且模型可能难以辨别“bank”是指银行还是河的一侧。</p>
<p>BERT 是双向的，它同时考虑左侧（“The bank is situated on the”）和右侧上下文（“of the river”），从而实现更细致的理解。它理解缺失的单词可能与银行的地理位置有关，展示了双向方法带来的语境丰富性。</p>
<p>BERT的关键创新点包括：</p>
<ol>
<li><p><strong>掩码语言建模（MLM）</strong>：BERT不是预测序列中的下一个单词，而是<strong>被训练来预测句子中随机掩码的词元</strong>。这迫使模型在进行预测时考虑句子的整个上下文——包括前面和后面的单词。例如，对于句子 “The cat sat on the [MASK] mat”，BERT会根据周围的上下文学习预测出 “soft”。</p>
</li>
<li><p><strong>下一句预测（NSP）</strong>：除了MLM，BERT还在一个名为下一句预测的次要任务上进行训练，在这个任务中，模型学习预测两个句子在文档中是否连续。这有助于BERT在需要理解句子之间关系的任务中表现出色，如问答和自然语言推理。</p>
</li>
</ol>
<p><img src= "/img/spin.svg" data-lazy-src="https://rean-blog-bucket.oss-cn-guangzhou.aliyuncs.com/assets/essay/image-20250724154631362.png"></p>
<h3 id="BERT-wwm（2019）"><a href="#BERT-wwm（2019）" class="headerlink" title="BERT-wwm（2019）"></a>BERT-wwm（2019）</h3><p>2019年，哈工大和科大讯飞联合发表<strong>中文BERT-WWM</strong>模型的论文。</p>
<div class="note  info flat"><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.08101">Pre-Training with Whole Word Masking for Chinese BERT</a></p>
</div>

<p>BERT-wwm 调整的地方在于<strong>MLM过程中使用全词掩码（Whole Word Mask）</strong>：</p>
<p>上述提到，BERT 使用 WordPiece 对句子进行切分 tokens，并且会产生 Subword，<strong>如果 Subword 被选中 mask，那么整个单词都会进行 mask</strong>，而原来的则不会。</p>
<p>例如：playing被拆分为 <code>play</code> 和 <code>##ing</code> 两个Subword，当 <code>##ing</code> 被选中mask时，那么 <code>play</code> 也会同时进行mask。</p>
<p>中文和英文不同，英文最小的 token 是一个单词，而中文中最小的 token 却是字，词是由一个或多个字组成，且每个词之间没有明显的分割，包含更多信息的是词，对全词 mask 就是对整个词都通过 mask 进行掩码。</p>
<p>例：</p>
<p>原始文本： 使用语言模型来预测下一个词的probability。<br>分词文本： 使用 语言 <strong>模型</strong> 来 <strong>预测</strong> 下 一个 词 的 <strong>probability</strong> 。<br>原始Mask输入：使用 语言 <strong>[MASK] 型</strong> 来 <strong>[MASK] 测</strong> 下 一个 词 的 <strong>pro [MASK] ##lity</strong> 。<br>全词Mask输入 使用 语言 <strong>[MASK] [MASK]</strong> 来 <strong>[MASK] [MASK]</strong> 下 一个 词 的 <strong>[MASK] [MASK] [MASK]</strong> 。</p>
<h3 id="RoBERTa（2019）"><a href="#RoBERTa（2019）" class="headerlink" title="RoBERTa（2019）"></a>RoBERTa（2019）</h3><div class="note  info flat"><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></p>
</div>

<p>RoBERTa 出自 Facebook，提出了一种鲁棒性更强的 BERT 模型预训练方法。RoBERTa 的预训练语料更多、Batch size更大、训练更久，具体改进如下：</p>
<p><strong>（1）静态Mask变动态Mask</strong></p>
<p>Bert在整个预训练过程，选择进行mask的15%的Tokens是不变的，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。</p>
<p>而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N&#x2F;10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。</p>
<p>这样做的目的是：动态mask相当于间接的增加了训练数据，有助于提高模型性能。</p>
<p><strong>（2）移去NSP任务</strong></p>
<p>Bert为了捕捉句子之间的关系，使用了NSP任务进行预训练，就是输入一对句子A和B，判断这两个句子是否是连续的。两句子最大长度之和为512。</p>
<p>RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL-SENTENCES），而原来的Bert每次只输入两个句子。</p>
<p>这样做的目的是：实验发现，消除NSP损失在下游任务的性能上能够与原始BERT持平或略有提高。这可能是由于Bert一单句子为单位输入，模型无法学习到词之间的远程依赖关系，而RoBERTa输入为连续的多个句子，模型更能俘获更长的依赖关系，这对长序列的下游任务比较友好。</p>
<h3 id="SpanBERT（2019）"><a href="#SpanBERT（2019）" class="headerlink" title="SpanBERT（2019）"></a>SpanBERT（2019）</h3><div class="note  info flat"><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.10529">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a></p>
</div>

<p>SpanBERT 出自Facebook，在 BERT 的基础上，针对预测 spans of text 的任务，在预训练阶段做了特定的优化。</p>
<p>SpanBERT所做的预训练调整主要是以下三点：</p>
<ul>
<li>使用一种<strong>span masking</strong>来代替BERT的mask；</li>
<li>加入另外一个新的训练目标：<strong>Span Boundary Objective (SBO)</strong></li>
<li><strong>使用单个句子而非一对句子，并且不使用Next Sentence Prediction任务。</strong></li>
</ul>
<p>span masking 每次都会通过采样文本（X）的一个片段（span），得到一个子集（Y∈X）。在每次采样过程中，首先，随机选取一个片段长度，然后再随机选取一个起点，这样就可以到一个 span 进行 mask 了；span 的长度会进行截断，即不超过10；另外，span 的长度是指 word 的长度，而不是 subword，这也意味着采样的单位是 word 而非 subword，并且随取的起点必须是一个 word 的开头。</p>
<h3 id="ERNIE（2019-3）"><a href="#ERNIE（2019-3）" class="headerlink" title="ERNIE（2019.3）"></a>ERNIE（2019.3）</h3><div class="note  info flat"><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09223">ERNIE: Enhanced Representation through Knowledge Integration</a></p>
</div>

<p><strong>百度文心1.0认为BERT模型没有考虑到句子里的先验知识。</strong>比如这个句子：哈尔滨是黑龙江的省会，国际冰雪文化名城。</p>
<ul>
<li>模型很容易通过「哈」与「滨」预测到「尔」这个单词，仅仅需要通过局部共现关系，完全不需要上下文的帮助；</li>
<li>而无法直接通过黑龙江与哈尔滨的关系来预测「哈尔滨」。</li>
</ul>
<p><img src= "/img/spin.svg" data-lazy-src="https://rean-blog-bucket.oss-cn-guangzhou.aliyuncs.com/assets/essay/v2-ff9423e1f52730574fa1451ff7861aed_r.jpg"></p>
<p><img src= "/img/spin.svg" data-lazy-src="https://rean-blog-bucket.oss-cn-guangzhou.aliyuncs.com/assets/essay/20250724162608.png"></p>
<p>因此，如果模型能够学到这些先验知识，那模型应该可以获得更可靠的语言表征。ERNIE的改进主要体现在mask的方式上，将中文单字的 mask 改为<strong>连续的实体词和短语</strong> mask。</p>
<p>另外，ERNIE 在 Bert 的基础上，预训练的语料引入了多源数据知识，包括了中文维基百科，百度百科，百度新闻和百度贴吧（可用于对话训练），数量分别为21M、51M、47M、54M。</p>
<p>并且，在<strong>中文使用了繁体字到简体字的转换</strong>，英文上的大写到小写的转换，最终的词表大小为17964。</p>
<p>之后清华也出了一个版本的ERNIE，它将<strong>知识图谱</strong>融入到语言模型的预训练之中，使用TransE来获取知识图谱中的实体向量，然后将实体向量嵌入到BERT中。</p>
<div class="note  info flat"><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.07129">ERNIE: Enhanced Language Representation with Informative Entities</a></p>
</div>

<p>比如，不知道《Blowin’ in the Wind》是歌曲、《Chronicles: Volume One》是书籍，就很难准确认出鲍勃・迪伦的词曲作者和作家这两个身份。ERNIE 为了解决这个问题，同时利用大规模文本语料和知识图谱来训练。</p>
<p><img src= "/img/spin.svg" data-lazy-src="https://rean-blog-bucket.oss-cn-guangzhou.aliyuncs.com/assets/essay/image-20250724163908062.png"></p>
<h2 id="GPT（2018-2020）"><a href="#GPT（2018-2020）" class="headerlink" title="GPT（2018 - 2020）"></a>GPT（2018 - 2020）</h2><p><img src= "/img/spin.svg" data-lazy-src="https://rean-blog-bucket.oss-cn-guangzhou.aliyuncs.com/assets/essay/16156e0a89568a97d3bffc06cd98674b.png"></p>
<h3 id="GPT-1（2018-6）"><a href="#GPT-1（2018-6）" class="headerlink" title="GPT-1（2018.6）"></a>GPT-1（2018.6）</h3><p>2017年，Google推出了Transformer模型，这一架构因其在性能上的显著优势迅速吸引了OpenAI团队的注意。OpenAI随后将研发重点转移到Transformer架构，并在2018年发布了GPT-1模型。GPT-1是基于生成式预训练（Generative Pre-Training）的Transformer架构，采用了<strong>仅有解码器（Decoder-Only）</strong>的Transformer模型，专注于预测下一个词元。</p>
<p>然而，GPT-1由于规模与BERT-Base相当，且在公开评测数据集上的性能未能达到最优，因此没有在学术界引起足够的关注。</p>
<p>特点：</p>
<ul>
<li><strong>单向自回归训练</strong>：GPT使用因果语言建模目标进行训练，在这种训练方式中，模型仅根据前面的词元预测下一个词元。这使得它特别适合文本完成、摘要和对话生成等生成任务。</li>
<li><strong>下游任务微调</strong>：GPT的关键贡献之一是它能够<strong>针对特定的下游任务进行微调</strong>，而无需特定任务的架构。通过简单地添加一个分类头或修改输入格式，GPT可以适应情感分析、机器翻译和问答等任务。</li>
</ul>
<h3 id="GPT-2（2019-2）"><a href="#GPT-2（2019-2）" class="headerlink" title="GPT-2（2019.2）"></a>GPT-2（2019.2）</h3><p>GPT-2继承了GPT-1的架构，并将参数规模扩大到<strong>15亿</strong>，使用大规模网页数据集WebText进行预训练。与GPT-1相比，GPT-2的创新之处在于尝试通过<strong>增加模型参数规模</strong>来提升性能，同时<strong>去除针对特定任务的微调环节</strong>，<strong>探索使用无监督预训练的语言模型</strong>来解决多种下游任务，而无需显式地使用标注数据进行微调（即zero-shot）。</p>
<p><img src= "/img/spin.svg" data-lazy-src="https://rean-blog-bucket.oss-cn-guangzhou.aliyuncs.com/assets/essay/20250724180111.png"></p>
<h3 id="GPT-3（2020-5）"><a href="#GPT-3（2020-5）" class="headerlink" title="GPT-3（2020.5）"></a>GPT-3（2020.5）</h3><p>OpenAI在2020年推出了具有里程碑意义的GPT-3模型，其模型参数规模扩展到了<strong>175B（1750亿）</strong>，相较于GPT-2提升了100余倍，标志着对模型扩展的极限尝试。</p>
<p>GPT-3首次提出了“上下文学习”概念，允许大语言模型通过少样本学习解决各种任务，消除了对新任务进行微调的需求。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642923989">LLM的3种架构：Encoder-only、Decoder-only、encode-decode - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://transformers.run/">Hello! · Transformers快速入门</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_63669388/article/details/131362133">史上最详细Transformer讲解以及transformer实现中文版完形填空（掩蔽字训练MASK） 内容详细易懂且附有全部代码_transformer mask-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42475060/article/details/121101749">【超详细】【原理篇&amp;实战篇】一文读懂Transformer-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/687832172">第四篇：一文搞懂Transformer架构的三种注意力机制 - 知乎 (zhihu.com)</a></p>
<p>[<a target="_blank" rel="noopener" href="https://blog.csdn.net/xinquanv1/article/details/136287616">ai笔记13] 大模型架构对比盘点：Encoder-Only、Decoder-Only、Encoder-Decoder_encoderonly和decoderonly的区别-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23720060587">大模型进化史：从Transformer到DeepSeek-R1的AI变革之路 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/598095233">BERT模型系列大全解读 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/166254682">BERT-WWM - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/145119424">万字长文带你纵览 BERT 家族 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_59235245/article/details/139782644">长文梳理！近年来GPT系列模型的发展历史：从GPT-1到GPT-4o（前世、今生）_gpt发展历程-CSDN博客</a></p>
</article><div class="post-copyright"><div class="post-copyright__author_group"><a class="post-copyright__author_img" href="/about/"><img class="post-copyright__author_img_front" src= "/img/spin.svg" data-lazy-src="/img/avatar.jpg"></a><div class="post-copyright__author_name">Rean's Blog</div><div class="post-copyright__author_desc">Enjoy technology and music</div></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div id="quit-box" onclick="RemoveRewardMask()"></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本文是原创文章，采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a>协议，完整转载请注明来自<a href="/">Rean's Blog</a></span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__tag-list"></div></div></div><nav class="needEndHide pagination-post" id="pagination"><div class="next-post pull-full"><a href="/p/694e526e/"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【Notes】LLM API+Agent+FastAPI+SSE+Docker</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><div class="author-info__top-group"><div class="author-info__sayhi" id="author-info__sayhi" onclick="sco.changeSayHelloText()">sayhello.morning</div></div></div><div class="avatar-img-group"><img class="avatar-img" alt="头像" src= "/img/spin.svg" data-lazy-src="/img/avatar.jpg"><div class="avatar-sticker"><img class="avatar-sticker-img" src= "/img/spin.svg" data-lazy-src="https://7.isyangs.cn/34/65f2e4e0423cc-34.png" alt="心情贴纸"></div></div><div class="author-info__description_group"><div class="author-info__description">分享自己对编程的<b>热爱</b>，对知识海洋<b>探索历程</b>，随机掉落<b>无参配布</b>以及<b>翻调</b>（主要是音综类，在B站因为版权基本过不了审（。</div><div class="author-info__description2">相信你可以在这里找到对你有用的知识和教程。</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/about/"><div class="author-info__name">Rean</div><div class="author-info__desc">Enjoy technology and music</div></a><div class="card-info-social-icons is-center"><a class="social-icon" target="_blank" rel="noopener" href="https://github.com/Rean-Schwarze" title="Github"><i class="solitude  st-github-line"></i></a><a class="social-icon" target="_blank" rel="noopener" href="https://space.bilibili.com/6531436" title="Bilibili"><i class="solitude  st-bilibili-line"></i></a></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="solitude st-menu-line"></i><span>文章目录</span></div><div class="toc-content" id="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%BF%9B%E5%8C%96%E5%8F%B2"><span class="toc-text">大模型进化史</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer%EF%BC%882017%EF%BC%89"><span class="toc-text">Transformer（2017）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B5%B7%E6%BA%90%E4%B8%8E%E5%8F%91%E5%B1%95"><span class="toc-text">起源与发展</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-text">基本结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder-Only"><span class="toc-text">Encoder-Only</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder-Only"><span class="toc-text">Decoder-Only</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder-Decoder"><span class="toc-text">Encoder-Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E7%82%B9"><span class="toc-text">特点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT%EF%BC%882018-10%EF%BC%89"><span class="toc-text">BERT（2018.10）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-wwm%EF%BC%882019%EF%BC%89"><span class="toc-text">BERT-wwm（2019）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RoBERTa%EF%BC%882019%EF%BC%89"><span class="toc-text">RoBERTa（2019）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SpanBERT%EF%BC%882019%EF%BC%89"><span class="toc-text">SpanBERT（2019）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ERNIE%EF%BC%882019-3%EF%BC%89"><span class="toc-text">ERNIE（2019.3）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPT%EF%BC%882018-2020%EF%BC%89"><span class="toc-text">GPT（2018 - 2020）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-1%EF%BC%882018-6%EF%BC%89"><span class="toc-text">GPT-1（2018.6）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-2%EF%BC%882019-2%EF%BC%89"><span class="toc-text">GPT-2（2019.2）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-3%EF%BC%882020-5%EF%BC%89"><span class="toc-text">GPT-3（2020.5）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-text">参考链接</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="solitude st-map-line"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/p/f2766b22/" title="【LLM】综述"><img alt="【LLM】综述" src= "/img/spin.svg" data-lazy-src="https://npm.elemecdn.com/justlovesmile-photo/cover5.JPG"></a><div class="content"><a class="title" href="/p/f2766b22/" title="【LLM】综述">【LLM】综述</a><a class="article-recent_post_categories" href="/p/f2766b22/">学习</a></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/694e526e/" title="【Notes】LLM API+Agent+FastAPI+SSE+Docker"><img alt="【Notes】LLM API+Agent+FastAPI+SSE+Docker" src= "/img/spin.svg" data-lazy-src="https://npm.elemecdn.com/justlovesmile-photo/cover8.JPG"></a><div class="content"><a class="title" href="/p/694e526e/" title="【Notes】LLM API+Agent+FastAPI+SSE+Docker">【Notes】LLM API+Agent+FastAPI+SSE+Docker</a><a class="article-recent_post_categories" href="/p/694e526e/">学习</a></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/d2c104ef/" title="【星尘Infinity/未抒】逆光（Cover：周深/陈楚生）【Synthesizer V Cover】"><img alt="【星尘Infinity/未抒】逆光（Cover：周深/陈楚生）【Synthesizer V Cover】" src= "/img/spin.svg" data-lazy-src="https://rean-song-bucket.oss-cn-guangzhou.aliyuncs.com/assets/cover/逆光_cover.png"></a><div class="content"><a class="title" href="/p/d2c104ef/" title="【星尘Infinity/未抒】逆光（Cover：周深/陈楚生）【Synthesizer V Cover】">【星尘Infinity/未抒】逆光（Cover：周深/陈楚生）【Synthesizer V Cover】</a><a class="article-recent_post_categories" href="/p/d2c104ef/">VOCALOID·UTAU</a></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/401d6e5a/" title="【永夜Minus】珠玉 (live)（Cover：单依纯）【歌手2025】【Synthesizer V Cover】"><img alt="【永夜Minus】珠玉 (live)（Cover：单依纯）【歌手2025】【Synthesizer V Cover】" src= "/img/spin.svg" data-lazy-src="https://rean-song-bucket.oss-cn-guangzhou.aliyuncs.com/assets/cover/珠玉_cover.jpg"></a><div class="content"><a class="title" href="/p/401d6e5a/" title="【永夜Minus】珠玉 (live)（Cover：单依纯）【歌手2025】【Synthesizer V Cover】">【永夜Minus】珠玉 (live)（Cover：单依纯）【歌手2025】【Synthesizer V Cover】</a><a class="article-recent_post_categories" href="/p/401d6e5a/">VOCALOID·UTAU</a></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/197094e3/" title="【墨清弦AI】唯一（Cover：G.E.M.邓紫棋）【ACE Cover】"><img alt="【墨清弦AI】唯一（Cover：G.E.M.邓紫棋）【ACE Cover】" src= "/img/spin.svg" data-lazy-src="https://rean-song-bucket.oss-cn-guangzhou.aliyuncs.com/assets/cover/唯一_cover.png"></a><div class="content"><a class="title" href="/p/197094e3/" title="【墨清弦AI】唯一（Cover：G.E.M.邓紫棋）【ACE Cover】">【墨清弦AI】唯一（Cover：G.E.M.邓紫棋）【ACE Cover】</a><a class="article-recent_post_categories" href="/p/197094e3/">VOCALOID·UTAU</a></div></div></div></div></div></div></main><footer id="footer"><div id="st-footer-bar"><div class="footer-logo"><span class="solitude">Rean's Blog</span></div><div class="footer-bar-description">来自Rean's Blog - Enjoy technology and music的文章</div><a class="footer-bar-link" href="/">了解更多</a></div><div id="footer_deal"><a class="deal_link" target="_blank" rel="noopener" href="https://github.com/Rean-Schwarze" title="Github"><i class="solitude  st-github-line"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://space.bilibili.com/6531436" title="Bilibili"><i class="solitude  st-bilibili-line"></i></a></div><div id="st-footer"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div class="copyright">© 2024 - 2025 By&nbsp;<a class="footer-bar-link" href="/">Rean</a></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/valor-x/hexo-theme-solitude" alt="主题">主题</a><a class="footer-bar-link cc" href="/null" aria-label="copyright"><i class="solitude st-copyright-line"></i><i class="solitude st-creative-commons-by-line"></i><i class="solitude st-creative-commons-nc-line"></i><i class="solitude st-creative-commons-nd-line"></i></a></div></div></div></footer></div><!-- right_menu--><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="solitude st-arrow-left-line"></i></div><div class="rightMenu-item" id="menu-forward"><i class="solitude st-arrow-right-line"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="solitude st-restart-line"></i></div><div class="rightMenu-item" id="menu-top"><i class="solitude st-arrow-up-line"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="solitude st-copy-fill"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="solitude st-clipboard-fill"></i><span>粘贴文本</span></div><div class="rightMenu-item" id="menu-newwindow"><i class="solitude st-window-fill"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="solitude st-link-line"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="solitude st-copy-fill"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="solitude st-download-cloud-fill"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-search"><i class="solitude st-search-line"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="solitude st-play-fill"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="solitude st-skip-back-fill"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="solitude st-skip-forward-fill"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="solitude st-copy-fill"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><div class="rightMenu-item" id="menu-randomPost" onclick="toRandomPost()||rm.hideRightMenu()"><i class="solitude st-signal-tower-fill"></i><span>随机短文</span></div><div class="rightMenu-item" onclick="pjax.loadUrl('/categories/')||rm.hideRightMenu()"><i class="solitude st-checkbox-multiple-blank-fill"></i><span>全部分类</span></div><div class="rightMenu-item" onclick="pjax.loadUrl('/tags/')||rm.hideRightMenu()"><i class="solitude st-price-tag-fill"></i><span>全部标签</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><div class="rightMenu-item" id="menu-darkmode" onclick="sco.switchDarkMode()"><i class="solitude st-moon-clear-fill"></i><span class="menu-darkmode-text">深色模式</span></div><div class="rightMenu-item" id="menu-translate"><i class="solitude st-panben-line"></i><span>轉為繁體</span></div></div></div><div id="rightmenu-mask"></div><!-- inject body--><div><script src="/js/utils.js?v=1.8.10"></script><script src="/js/main.js?v=1.8.10"></script><script src="/js/third_party/waterfall.min.js?v=1.8.10"></script><script src="https://cdn.staticfile.net/pjax/0.2.8/pjax.min.js"></script><script src="/js/third_party/universe.min.js?v=1.8.10"></script><script>dark()
</script><script src="/js/tw_cn.js?v=1.8.10"></script><script src="https://cdn.staticfile.net/vanilla-lazyload/19.1.3/lazyload.iife.min.js"></script><script src="https://cdn.staticfile.net/node-snackbar/0.1.16/snackbar.min.js"></script><script src="https://cdn.staticfile.net/fancyapps-ui/5.0.36/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.staticfile.net/Swiper/11.0.5/swiper-bundle.min.js"></script><script>var meting_api = 'https://meting.qjqq.cn/?server=:server&type=:type&id=:id&auth=:auth&r=:r';</script><script src="https://cdn.staticfile.net/aplayer/1.10.1/APlayer.min.js"></script><script src="https://cdn.staticfile.net/meting/2.0.1/Meting.min.js"></script><script>const coverColorConfig = {
    api: 'https://api.qjqq.cn/api/Imgcolor?img=',
    time: 43200000
}</script><script src="/js/covercolor/api.js?v=1.8.10"></script><script src="/js/music.js?v=1.8.10"></script><script src="https://cdn.staticfile.net/pace/1.2.4/pace.min.js"></script><script src="/js/right_menu.js?v=1.8.10"></script><div class="js-pjax"><script defer pjax src="https://cdn.staticfile.net/pearssauce-busuanzi/1.0.0/bsz.pure.min.js"></script></div></div><!-- newest comment--><!-- pjax--><script>const pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: ['title','#body-wrap','#site-config','meta[name="description"]','.js-pjax','meta[property^="og:"]','#config-diff'],
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
})

document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
})

document.addEventListener('pjax:complete', () => {
    window.refreshFn()

    document.querySelectorAll('script[data-pjax]').forEach(item => {
        const newScript = document.createElement('script')
        const content = item.text || item.textContent || item.innerHTML || ""
        Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
        newScript.appendChild(document.createTextNode(content))
        item.parentNode.replaceChild(newScript, item)
    })

    GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

})

document.addEventListener('pjax:error', (e) => {
    if (e.request.status === 404) {
        pjax.loadUrl('/404.html')
    }
})</script><!-- theme--><script>initTheme = () => {
    let isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const cachedMode = utils.saveToLocal.get('theme');
    if (cachedMode === undefined) {
        const nowMode =
            isDarkMode ? 'dark' : 'light'
        document.documentElement.setAttribute('data-theme', nowMode);
    } else {
        document.documentElement.setAttribute('data-theme', cachedMode);
    }
    is_rm && rm.mode(cachedMode === 'dark' && isDarkMode)
}
initTheme()</script><!-- google adsense--><!-- search--><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="solitude st-close-fill"></i></button></nav><div class="search-wrap"><div class="search-box"><input class="search-box-input" id="search-input" type="text" autocomplete="off" spellcheck="false" autocorrect="off" autocapitalize="off" placeholder="输入关键词快速查找"></div><div id="search-results"><div id="search-hits"></div></div><div id="search-pagination"></div><div id="search-tips"></div></div></div><div id="search-mask"></div></div><script src="/js/search/local.js?v=1.8.10"></script><!-- music--><div class="needEndHide" id="nav-music" onclick="sco.musicToggle()"><div id="nav-music-hoverTips">音乐已暂停</div><meting-js id="421801417" server="netease" type="playlist" mutex="true" preload="none" data-lrctype="0" order="random" theme="var(--efu-main)"></meting-js></div></body></html><script>const posts=["p/f2766b22/","p/694e526e/","p/d2c104ef/","p/401d6e5a/","p/197094e3/","p/867e546a/","p/9d906c2f/","p/8e8e2cc6/","p/2b6e13f/","p/e7845a78/","p/5871d67/","p/204d99f1/","p/b585efd0/","p/122902e7/","p/683f7c8e/","p/36b252a1/","p/bbd98f0d/","p/1b320550/","p/30cd73ca/","p/332adb42/","p/4f877c1e/","p/677c7789/","p/f7f0a5f3/","p/9b5a538c/","p/37360bc0/","p/bb996d5/","p/364ea8cc/","p/a027699d/","p/21cdaec5/","p/37c37880/","p/a02febe4/","p/d1ec0e4e/","p/25d2b878/","p/8324e4bc/","p/6021f67a/","p/305331f0/","p/66be4b4/","p/de2d10bc/","p/e6628f8e/","p/4131eeff/","p/27705e17/","p/74e1a37c/","p/b5223f80/","p/9b8bc45a/","p/c9cb7159/","p/82c942ab/","p/4c19fcc6/","p/27446c6a/","p/9f38852a/","p/2516f5f2/","p/b27c77b3/","p/7a3264f/","p/7beadfaa/","p/e1d9906f/","p/a440b512/","p/a46aa116/","p/fe8dd3bd/","p/51c99199/","p/91b418a/","p/421add40/","p/9d7a18c7/","p/26345198/","p/fa30e874/","p/dea9a817/","p/59203ba6/","p/e5616c72/","p/971e9859/","p/d7f6ccff/","p/1b2ec1c2/","p/a0d1cef9/","p/a58b6af0/","p/9381aa6d/","p/38b8becb/","p/d0a26095/","p/2b3bdada/","p/ba70636/","p/374630f5/","p/a964a243/","p/a06e9899/","p/ef9a99de/","p/108f7c52/","p/a91c1846/","p/413576cf/","p/9ee60c65/","p/e74db628/","p/888c8bf3/","p/477dbe08/","p/38e8a510/","p/dfb41864/","p/b0e3506b/","p/e17fd725/","p/e1fe458c/","p/636cfdea/","p/7b854cb7/","p/b3a61240/","p/64f43fb4/","p/800f9a11/","p/4a48b34c/","p/b3641215/","p/53bc1fc9/","p/fc02f9ee/","p/25b929f6/","p/dd30b42/","p/3aa261db/","p/fc078742/","p/22c9d1d0/","p/33fc02d2/","p/70335ae0/","p/f32ef110/","p/f756486c/","p/8dd4b91f/","p/a290d941/","p/61a8a4ef/","p/c2d0bb20/","p/9770d71c/","p/41bd2aed/","p/bfebe40b/","p/6b4006e6/","p/b5696360/","p/53ab7115/","p/e6fa9b9/","p/866abf45/","p/b718b92a/","p/d26f4c21/","p/bc5d6bbe/","p/5a18917b/","p/5f2b1e2f/","p/2532a2ee/","p/4337b2c0/","p/e042d029/","p/9b4a91e5/","p/9d8ce868/","p/a69f6a48/","p/f9b9edb2/","p/cbe55d0/","p/156c59d0/","p/d4d6f818/","p/2574cf15/","p/a0efc292/","p/ec2ecf28/","p/85f11265/","p/db95e6f5/","p/a8296e6d/","p/4cc89a71/","p/d0fe2d0f/","p/ffab2f6b/","p/95d9f91e/","p/7f869365/","p/5b246b1b/","p/a9c92fa9/","p/7d49fec0/","p/a9a25fcd/","p/43bb383e/","p/b9aeb4/","p/52eaf228/","p/4be90d2f/","p/ed500d41/","p/dcfbc8e5/","p/240e5e52/","p/18cda1b3/","p/66a6c396/","p/5f33341/","p/f9bef31e/","p/acac46b/","p/a4189438/","p/ae6b0147/","p/4e982f2d/","p/403e0cac/","p/495b9f0f/","p/8e966ab9/","p/f6e1007e/","p/6f562d19/","p/10eac329/","p/cc53cf58/","p/4f2beac6/","p/73e2aa1/","p/c677e75e/","p/3f13b766/","p/cc30f00/","p/807cab5e/","p/5b5991e4/","p/e941ffc/"];function toRandomPost(){ pjax.loadUrl('/'+posts[Math.floor(Math.random()*posts.length)]); }</script>